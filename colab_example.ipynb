{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {
    "id": "2e2e251b"
   },
   "source": [
    "# Hogwild! Parallelism: Minimal Example (Colab Edition) [![Code](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/eqimp/hogwild_llm) [![arxiv](https://camo.githubusercontent.com/ce27cdf7b9627a67089c7bec66b101c90c6bbf21c2452a067a5bb5a4eac40d58/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41725869762d5044462d726564)](https://arxiv.org/abs/2504.06261)\n",
    "\n",
    "This notebook demonstrates Hogwild! inference on a single problem with 2 workers and **using a small model to fit into colab's T4 GPU**. The smaller model can, to some extent, collaborate, but not as well as the larger reasoning-tuned LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eakaN6WyU4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eakaN6WyU4d",
    "outputId": "03bbb13d-3a56-437f-f264-414a75c3b990"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/eqimp/hogwild_llm && cp -r hogwild_llm/* .\n",
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from generation import MathFormatting, get_logits_processor\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "# load the smaller model to fit in colab; If you have a larger GPU, load QwQ-32B or R1 for more reliable collaboration\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354601ba",
   "metadata": {
    "id": "354601ba"
   },
   "source": [
    "__Playground:__ you can define a problem and see if the workers collaborate. As we state earlier, small models like this one often fail in silly ways when they try to interact. Use QwQ-32B similar for better effect. Though, they clearly *try* to colaborate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadbc3c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aadbc3c8",
    "outputId": "41e7337d-e619-4bcf-f571-cfe964021f1a"
   },
   "outputs": [],
   "source": [
    "problem = \"\"\"Calculate x - x^2 + x * (1 - x) for x = 4,5,6,7.\"\"\".strip()\n",
    "\n",
    "print_every_steps = 3\n",
    "insert_s1_prompt_every_tokens = 512\n",
    "tokens_since_last_wait = 0\n",
    "\n",
    "workers = [\"Alice\", \"Bob\"]\n",
    "fmt = MathFormatting(\n",
    "    tokenizer, workers, pass_system_prompt_as_user_message=True, wrap_shots_with_chat_template=True\n",
    ")  # ^-- prompts and optional few-shot examples; has options for different model types - see formatting.py\n",
    "worker_prompts = [\n",
    "    f\"\"\"{fmt.get_step_prefix(workers[0], 1)}Hi, I'm {workers[0]}. Here's how we should collaborate:\"\"\",\n",
    "    f\"\"\"{fmt.get_step_prefix(workers[1], 1)}Hi, I'm {workers[1]}.\"\"\"\n",
    "]\n",
    "\n",
    "# define cache structure for the combined layout\n",
    "cache_common, cache_current_step_header, cache_separator, cache_w1, cache_w2 = (\n",
    "    shared_cache.CacheBlock(config=model.config) for _ in range(5))\n",
    "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
    "    [cache_common, cache_current_step_header, cache_w2, cache_separator, cache_w1],\n",
    "    [cache_common, cache_current_step_header, cache_w1, cache_separator, cache_w2],\n",
    "])\n",
    "\n",
    "logits_processor = get_logits_processor(model, fmt.forbidden_token_ix)\n",
    "tokenizer_kwargs = dict(return_tensors='pt', padding=True, padding_side='left', add_special_tokens=False)\n",
    "\n",
    "# initialize generation state for printing\n",
    "history = []\n",
    "current_step_index_by_worker = [1, 1]\n",
    "current_step_tokens_by_worker = [tokenizer.encode(p, add_special_tokens=False) for p in worker_prompts]\n",
    "\n",
    "# pre-fill common parts\n",
    "with torch.inference_mode():\n",
    "    prompt_cm = shared_cache.SharedCacheManager(cache_structure=[[cache_common]])\n",
    "    prompt_inputs = tokenizer(fmt.get_full_prompt(problem), **tokenizer_kwargs)\n",
    "    model(**prompt_cm.get_input_kwargs(**{k: v[..., :3072].to(device) for k, v in prompt_inputs.items()}))\n",
    "    model(**prompt_cm.get_input_kwargs(**{k: v[..., 3072:].to(device) for k, v in prompt_inputs.items()}))\n",
    "    # ^-- process the prompt in two passes to avoid GPU OOM\n",
    "    model(**tokenizer(fmt.current_step_header, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_current_step_header);   # <-- write to separator\n",
    "    model(**tokenizer(fmt.current_worker_header, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_separator);   # <-- write to separator between incomplete steps\n",
    "\n",
    "next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uyS3sNHMF_j8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "uyS3sNHMF_j8",
    "outputId": "b722c848-1f5f-4dac-e178-90b8965b770b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[**Problem:** Calculate x - x^2 + x * (1 - x) for x = 4,5,6,7.]\n",
       "\n",
       "### Past steps\n",
       "\n",
       "**Bob [1]:** Hi, I'm Bob. Let's compute the value of x - x² + x(1 - x) for x = 5 and 7.\n",
       "\n",
       "**Alice [1]:** Hi, I'm Alice. Here's how we should collaborate: I'll handle the computation for x=4, 6, and you can handle x=5 and 7. I'll start with x=4.\n",
       "\n",
       "**Bob [2]:**  Let me calculate x - x² + x(1 - x) for x = 5.\n",
       "\n",
       "**Alice [2]:**  For x=4: 4 - 4² + 4(1 - 4) = 4 - 16 + 4(-3) = 4 - 16 - 12 = -24.\n",
       "\n",
       "**Bob [3]:** 5 - 5² + 5(1 - 5) = 5 - 25 + 5(-4) = 5 - 25 - 20 = -40.\n",
       "\n",
       "**Alice [3]:**  For x=6: 6 - 6² + 6(1 - 6) = 6 - 36 + 6(-5) = 6 - 36 - 30 = -60. \n",
       "\n",
       "**Bob [4]:** 6 - 6² + 6(1 - 6) = 6 - 36 + 6(-5) = 6 - 36 - 30 = -60. \n",
       "\n",
       "\n",
       "\n",
       "### Work in progress (others)\n",
       "\n",
       "**Alice [4]:**  For x=7<...>\n",
       "\n",
       "**Bob [5]:** 7 -<...>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inference_step in range(128):  # <-- modify the number of generation steps here\n",
    "    # run model with shared cache\n",
    "    with torch.inference_mode():\n",
    "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        logits = logits_processor(next_inputs['input_ids'], logits)\n",
    "        new_tokens = torch.multinomial(logits.softmax(dim=-1), 1).flatten(\n",
    "        ) if model.generation_config.do_sample else logits.argmax(-1)\n",
    "\n",
    "    # process generated tokens for printing; handle step change, update next_inputs\n",
    "    assert len(new_tokens) == len(fmt.workers)\n",
    "    next_input_tokens = new_tokens.unsqueeze(-1).tolist()\n",
    "    for worker_index, (worker_name, worker_tokens, new_token) in enumerate(\n",
    "            zip(fmt.workers, current_step_tokens_by_worker, new_tokens.tolist())):\n",
    "        worker_tokens.append(new_token)\n",
    "        if fmt.is_end_of_step(worker_tokens):\n",
    "            # worker just finished their step - add it to common history and start a new step\n",
    "            current_step_index_by_worker[worker_index] += 1\n",
    "            history.extend(worker_tokens)\n",
    "            worker_tokens.clear()\n",
    "            start_msg = fmt.get_step_prefix(worker_name, current_step_index_by_worker[worker_index])\n",
    "            if tokens_since_last_wait > insert_s1_prompt_every_tokens:\n",
    "                start_msg += fmt.s1_collab_message   # <-- insert \"Wait, am I doing redundant work?\"\n",
    "                tokens_since_last_wait = 0\n",
    "            worker_tokens.extend(tokenizer.encode(start_msg, add_special_tokens=False))\n",
    "            cache_common.append_from(cm.cache_structure[worker_index][-1])\n",
    "            cm.cache_structure[worker_index][-1].clear()\n",
    "            next_input_tokens[worker_index] = [new_token] + worker_tokens\n",
    "        tokens_since_last_wait += len(next_input_tokens[worker_index])\n",
    "    next_inputs = tokenizer.pad(dict(input_ids=next_input_tokens), padding_side='left', return_tensors='pt').to(device)\n",
    "\n",
    "    if inference_step % print_every_steps == 0:\n",
    "        clear_output(True)  # # display current progress\n",
    "        output_parts = [f\"[**Problem:** {problem}]\\n\\n\"]\n",
    "        output_parts.append(fmt.history_header + fmt.sep + tokenizer.decode(history))\n",
    "        output_parts.append(fmt.current_step_header)\n",
    "        for worker_index, worker_tokens in enumerate(current_step_tokens_by_worker):\n",
    "            output_parts.append(tokenizer.decode(worker_tokens) + fmt.pivot_message + fmt.sep)\n",
    "        display(Markdown(''.join(output_parts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OtFO0CaID5DI",
   "metadata": {
    "id": "OtFO0CaID5DI"
   },
   "source": [
    "**Disclaimer: small models are poor collaborators** and may be incapable of more complex interactions required for LIMO - and sometimes fail to keep their own promises, doing redundant work despite agreeing not to. We recommend using larger models such as [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) when possible - they work significantly better together."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
