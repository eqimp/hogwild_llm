{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e2e251b",
      "metadata": {
        "id": "2e2e251b"
      },
      "source": [
        "# Hogwild! Parallelism: Minimal Example (Colab Edition) [![Code](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/eqimp/hogwild_llm) [![arxiv](https://camo.githubusercontent.com/ce27cdf7b9627a67089c7bec66b101c90c6bbf21c2452a067a5bb5a4eac40d58/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41725869762d5044462d726564)](https://arxiv.org/abs/2504.06261)\n",
        "\n",
        "This notebook demonstrates Hogwild! inference on a single problem with 2 workers and **using a small model to fit into colab's T4 GPU**. The smaller model can, to some extent, collaborate, but not as well as the larger reasoning-tuned LLMs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eqimp/hogwild_llm && cp -r hogwild_llm/* .\n",
        "import torch\n",
        "import transformers\n",
        "import shared_cache\n",
        "from utils import get_math_input_prompts, get_logits_processor\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "# load the smaller model to fit in colab; If you have a larger GPU, load QwQ-32B or R1 for more reliable collaboration\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
      ],
      "metadata": {
        "id": "1eakaN6WyU4d",
        "outputId": "0780a649-330b-4d6f-876a-18dac04e5e8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "449dbaab142843f6916552828d6ed14f",
            "697f5d0294144961bf1f962546d20a40",
            "3db4ffc50f734ae6a7e50b276a9fa797",
            "4ef8252be8574a5fa5fdf79e2eeee4db",
            "3da9a20fffef475a843337c33c7354ce",
            "bb5bee6e5efc4922875ed179365c5072",
            "4025cf4e72e64a32a8b90d32c9b943f1",
            "11e9f6c644a444d7a65417c6e77d8d7f",
            "f7201893771346c6a00ff528abba56a0",
            "088c7f8595c64608bee45939e75bb4b7",
            "c3cdd8fc2acf4834bdf3932aea5ffdc5",
            "8e4696619bfd4d9faf8efbc49367d3f2",
            "c9310f1466c24164985e5ccbd47f022f",
            "333327cf0ada4543b9f7ddc199f6b6ed",
            "e6c6fa84221845349a5bcde5094a7f89",
            "d03ec506574942b5aaba1cb415fb1025",
            "e1b1fce07b6b4d3c98280281ef5d2080",
            "41fccf99238c4c1aa351ed7e3f054d3e",
            "435621fc454f4e949beb65bda1a3d826",
            "bd321b0248694de58408108a18ac9c41",
            "6ac3efd808a24361bf7a4a848b950b29",
            "32afeff165fe44a3a514b07ef8218836",
            "bd5c5f93d08b4e3eb2895c07e0a9c671",
            "65fa5578b5ab4940b23d648507cdb7d5",
            "c2ce8af725ad4fecae85b55d992a748c",
            "bb78ab5dae9c40b580f75d4805e3e297",
            "a79ed70493f1401683b87275d3278164",
            "482e3f3ef92a407096e042e15c5f0bd3",
            "11925d32f1944d629f2c503bc3e5373c",
            "ef4b68f9f7284c3eb632dbaf3ff64740",
            "0cf83815613b49fb85a52350bf4a0043",
            "3472ab1d8a754b3f9a72ec4b5e7195e9",
            "2a3c3712cc824f58bb1fe58d16231712",
            "811cee8dc3484014aeb669cffd7bd621",
            "8137866fa4094b1790ba5ad201c438ff",
            "e31e999dad7b4aea89dae424acdb0ca5",
            "496d33fefd054175b0b43bc6e7e01e50",
            "fb738cf881f14d2b8f02c2556548fd17",
            "25bb149608084fe7a318964f3de8cbd6",
            "c898837e568e4a40b53645299a3442cd",
            "ff19866741604564adae30bd9cbdef9a",
            "9c18e65c00464c4ea85e6be0151b6a23",
            "615e3ba82532484880af113884a56eb3",
            "48b725f6dabb4a81bb49e340ac1950ee",
            "d72b859f510c458983dc80b7fec6d785",
            "bf95f2725298479c9449c865cc03b8ec",
            "14fe179b04614ff5ada7a4803aba87fd",
            "aa02c055c3924817911a53f59b942147",
            "7821f48870064621a62ecefcc6d73433",
            "42f18653f0534388bd6903a57d3d9644",
            "cc04a8703b654b1c94f0f146df001df5",
            "18dfb475399944229c82c9eb86921f7f",
            "d9902e38075a4a8d8e1ec984d51a3e26",
            "c087f703c8374a65b2d1726df8ec7ad1",
            "7b603199970b4e4eb9d9621352697897",
            "a61996201ff74b8fbb8f71faef45ddaa",
            "e898eefdbb0a4eca8ae5daebc572a74c",
            "896580569582419584a0dfa8f6d3d3d3",
            "8d009e4c215a4dd396e51c6a53577cae",
            "cb17bfe4ab8f4236bda8be4679e3a275",
            "f7c49cb7f971479dafa50b2601791deb",
            "080da107f1d8487298d180e35b68800a",
            "117566ac57b54ebfbc2d48da49f754cf",
            "c30bf8a4f7694e85ac7666807a5f2414",
            "700529ceda694c4cbe516fb85434e02c",
            "13fccdaf39ce414298cdca1898d5beb1"
          ]
        }
      },
      "id": "1eakaN6WyU4d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hogwild_llm'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 164 (delta 74), reused 119 (delta 41), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (164/164), 1.75 MiB | 8.39 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "449dbaab142843f6916552828d6ed14f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e4696619bfd4d9faf8efbc49367d3f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd5c5f93d08b4e3eb2895c07e0a9c671"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/945 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "811cee8dc3484014aeb669cffd7bd621"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d72b859f510c458983dc80b7fec6d785"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a61996201ff74b8fbb8f71faef45ddaa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354601ba",
      "metadata": {
        "id": "354601ba"
      },
      "source": [
        "__Playground:__ you can define a problem and see if the workers collaborate. As we state earlier, small models like this one often fail in silly ways when they try to interact. Use QwQ-32B similar for better effect. Though, they clearly *try* to colaborate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadbc3c8",
      "metadata": {
        "id": "aadbc3c8"
      },
      "outputs": [],
      "source": [
        "problem = \"\"\"Calculate x - x^2 + x * (1 - x) for x = 4,5,6,7.\"\"\".strip()\n",
        "\n",
        "print_every_steps = 3\n",
        "insert_s1_prompt_every_tokens = 512\n",
        "tokens_since_last_wait = 0\n",
        "\n",
        "workers = [\"Alice\", \"Bob\"]\n",
        "Formatting = get_math_input_prompts(tokenizer, workers)  # <-- prompts are defined here\n",
        "worker_prompts = [\n",
        "    f\"\"\"{Formatting.get_step_prefix(workers[0], 1)}Hi, I'm {workers[0]}. Here's how we should collaborate:\"\"\",\n",
        "    f\"\"\"{Formatting.get_step_prefix(workers[1], 1)}Hi, I'm {workers[1]}.\"\"\"\n",
        "]\n",
        "\n",
        "# define cache structure for the combined layout\n",
        "cache_common, cache_current_step_header, cache_separator, cache_w1, cache_w2 = (\n",
        "    shared_cache.CacheBlock(config=model.config) for _ in range(5))\n",
        "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
        "    [cache_common, cache_current_step_header, cache_w2, cache_separator, cache_w1],\n",
        "    [cache_common, cache_current_step_header, cache_w1, cache_separator, cache_w2],\n",
        "])\n",
        "\n",
        "logits_processor = get_logits_processor(model, Formatting.forbidden_token_ix)\n",
        "tokenizer_kwargs = dict(return_tensors='pt', padding=True, padding_side='left', add_special_tokens=False)\n",
        "\n",
        "# initialize generation state for printing\n",
        "history = []\n",
        "current_step_index_by_worker = [1, 1]\n",
        "current_step_tokens_by_worker = [tokenizer.encode(p, add_special_tokens=False) for p in worker_prompts]\n",
        "\n",
        "# pre-fill common parts\n",
        "with torch.inference_mode():\n",
        "    model(**tokenizer([Formatting.get_full_prompt(problem)], **tokenizer_kwargs).to(device),\n",
        "          use_cache=True, past_key_values=cache_common);  # <-- write to common prompt\n",
        "    model(**tokenizer(Formatting.current_step_header, **tokenizer_kwargs).to(device),\n",
        "          use_cache=True, past_key_values=cache_current_step_header);   # <-- write to separator\n",
        "    model(**tokenizer(Formatting.current_worker_header, **tokenizer_kwargs).to(device),\n",
        "          use_cache=True, past_key_values=cache_separator);   # <-- write to separator between incomplete steps\n",
        "\n",
        "next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inference_step in range(128):  # <-- modify the number of generation steps here\n",
        "    # run model with shared cache\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
        "        logits = logits_processor(next_inputs['input_ids'], logits)\n",
        "        new_tokens = torch.multinomial(logits.softmax(dim=-1), 1).flatten(\n",
        "        ) if model.generation_config.do_sample else logits.argmax(-1)\n",
        "\n",
        "    # process generated tokens for printing; handle step change, update next_inputs\n",
        "    assert len(new_tokens) == len(Formatting.workers)\n",
        "    next_input_tokens = new_tokens.unsqueeze(-1).tolist()\n",
        "    for worker_index, (worker_name, worker_tokens, new_token) in enumerate(\n",
        "            zip(Formatting.workers, current_step_tokens_by_worker, new_tokens.tolist())):\n",
        "        worker_tokens.append(new_token)\n",
        "        if Formatting.is_end_of_step(worker_tokens):\n",
        "            # worker just finished their step - add it to common history and start a new step\n",
        "            current_step_index_by_worker[worker_index] += 1\n",
        "            history.extend(worker_tokens)\n",
        "            worker_tokens.clear()\n",
        "            start_msg = Formatting.get_step_prefix(worker_name, current_step_index_by_worker[worker_index])\n",
        "            if tokens_since_last_wait > insert_s1_prompt_every_tokens:\n",
        "                start_msg += Formatting.s1_collab_message   # <-- insert \"Wait, am I doing redundant work?\"\n",
        "                tokens_since_last_wait = 0\n",
        "            worker_tokens.extend(tokenizer.encode(start_msg, add_special_tokens=False))\n",
        "            cache_common.append_from(cm.cache_structure[worker_index][-1])\n",
        "            cm.cache_structure[worker_index][-1].clear()\n",
        "            next_input_tokens[worker_index] = [new_token] + worker_tokens\n",
        "        tokens_since_last_wait += len(next_input_tokens[worker_index])\n",
        "    next_inputs = tokenizer.pad(dict(input_ids=next_input_tokens), padding_side='left', return_tensors='pt').to(device)\n",
        "\n",
        "    if inference_step % print_every_steps == 0:\n",
        "        clear_output(True)  # # display current progress\n",
        "        output_parts = [f\"[**Problem:** {problem}]\\n\\n\"]\n",
        "        output_parts.append(Formatting.history_header + Formatting.SEP + tokenizer.decode(history))\n",
        "        output_parts.append(Formatting.current_step_header)\n",
        "        for worker_index, worker_tokens in enumerate(current_step_tokens_by_worker):\n",
        "            output_parts.append(tokenizer.decode(worker_tokens) + Formatting.pivot_message + Formatting.SEP)\n",
        "        display(Markdown(''.join(output_parts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uyS3sNHMF_j8",
        "outputId": "dc2ae077-1170-4d66-e08e-de29e66ae732"
      },
      "id": "uyS3sNHMF_j8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[**Problem:** Calculate x - x^2 + x * (1 - x) for x = 4,5,6,7.]\n\n### Past steps\n\n**Alice [1]:** Hi, I'm Alice. Here's how we should collaborate: I'll calculate x - x² + x(1 - x) for x = 4, 5, 6, 7, and you can do the same for x = 3, 8, 9, 10. \n\n**Bob [1]:** Hi, I'm Bob. Let's start by calculating the expression for each x value: x - x² + x(1 - x) = x - x² + x - x². Simplifying, we get x - 2x². So we have two terms: x and -2x². \n\n**Alice [2]:** 4 - 2*4² = 4 - 32 = -28. \n\n**Bob [2]:** 5 - 2*5² = 5 - 50 = -45.\n\n**Alice [3]:** 6 - 2*6² = 6 - 72 = -66.\n\n\n\n### Work in progress (others)\n\n**Alice [4]:** 8 - 2*<...>\n\n**Bob [3]:** 7 - 2*7² = 7 - 98 =<...>\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer: small models are poor collaborators** and may be incapable of more complex interactions required for LIMO - and sometimes fail to keep their own promises, doing redundant work despite agreeing not to. We recommend using larger models such as [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) when possible - they work significantly better together."
      ],
      "metadata": {
        "id": "OtFO0CaID5DI"
      },
      "id": "OtFO0CaID5DI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
