{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e2e251b",
      "metadata": {
        "id": "2e2e251b"
      },
      "source": [
        "# Hogwild! Parallelism: Minimal Example (Colab Edition) [![Code](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/eqimp/hogwild_llm) [![arxiv](https://camo.githubusercontent.com/ce27cdf7b9627a67089c7bec66b101c90c6bbf21c2452a067a5bb5a4eac40d58/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41725869762d5044462d726564)](https://arxiv.org/abs/2504.06261)\n",
        "\n",
        "This notebook demonstrates Hogwild! inference on a single problem with 2 workers and **using a small model to fit into colab's T4 GPU**. The smaller model can, to some extent, collaborate, but not as well as the larger reasoning-tuned LLMs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eqimp/hogwild_llm && cp -r hogwild_llm/* .\n",
        "import torch\n",
        "import transformers\n",
        "import shared_cache\n",
        "from utils import get_math_input_prompts, get_logits_processor\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "# load the smaller model to fit in colab; If you have a larger GPU, load QwQ-32B or R1 for more reliable collaboration\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
      ],
      "metadata": {
        "id": "1eakaN6WyU4d",
        "outputId": "03bbb13d-3a56-437f-f264-414a75c3b990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1eakaN6WyU4d",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354601ba",
      "metadata": {
        "id": "354601ba"
      },
      "source": [
        "__Playground:__ you can define a problem and see if the workers collaborate. As we state earlier, small models like this one often fail in silly ways when they try to interact. Use QwQ-32B similar for better effect. Though, they clearly *try* to colaborate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aadbc3c8",
      "metadata": {
        "id": "aadbc3c8",
        "outputId": "41e7337d-e619-4bcf-f571-cfe964021f1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "problem = \"\"\"Calculate x - x^2 + x * (1 - x) for x = 4,5,6,7.\"\"\".strip()\n",
        "\n",
        "print_every_steps = 3\n",
        "insert_s1_prompt_every_tokens = 512\n",
        "tokens_since_last_wait = 0\n",
        "\n",
        "workers = [\"Alice\", \"Bob\"]\n",
        "Formatting = get_math_input_prompts(tokenizer, workers)  # <-- prompts are defined here\n",
        "worker_prompts = [\n",
        "    f\"\"\"{Formatting.get_step_prefix(workers[0], 1)}Hi, I'm {workers[0]}. Here's how we should collaborate:\"\"\",\n",
        "    f\"\"\"{Formatting.get_step_prefix(workers[1], 1)}Hi, I'm {workers[1]}.\"\"\"\n",
        "]\n",
        "\n",
        "# define cache structure for the combined layout\n",
        "cache_common, cache_current_step_header, cache_separator, cache_w1, cache_w2 = (\n",
        "    shared_cache.CacheBlock(config=model.config) for _ in range(5))\n",
        "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
        "    [cache_common, cache_current_step_header, cache_w2, cache_separator, cache_w1],\n",
        "    [cache_common, cache_current_step_header, cache_w1, cache_separator, cache_w2],\n",
        "])\n",
        "\n",
        "logits_processor = get_logits_processor(model, Formatting.forbidden_token_ix)\n",
        "tokenizer_kwargs = dict(return_tensors='pt', padding=True, padding_side='left', add_special_tokens=False)\n",
        "\n",
        "# initialize generation state for printing\n",
        "history = []\n",
        "current_step_index_by_worker = [1, 1]\n",
        "current_step_tokens_by_worker = [tokenizer.encode(p, add_special_tokens=False) for p in worker_prompts]\n",
        "\n",
        "# pre-fill common parts\n",
        "with torch.inference_mode():\n",
        "    prompt_cm = shared_cache.SharedCacheManager(cache_structure=[[cache_common]])\n",
        "    prompt_inputs = tokenizer(Formatting.get_full_prompt(problem), **tokenizer_kwargs)\n",
        "    model(**prompt_cm.get_input_kwargs(**{k: v[..., :3072].to(device) for k, v in prompt_inputs.items()}))\n",
        "    model(**prompt_cm.get_input_kwargs(**{k: v[..., 3072:].to(device) for k, v in prompt_inputs.items()}))\n",
        "    model(**tokenizer(Formatting.current_step_header, **tokenizer_kwargs).to(device),\n",
        "          use_cache=True, past_key_values=cache_current_step_header);   # <-- write to separator\n",
        "    model(**tokenizer(Formatting.current_worker_header, **tokenizer_kwargs).to(device),\n",
        "          use_cache=True, past_key_values=cache_separator);   # <-- write to separator between incomplete steps\n",
        "\n",
        "next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inference_step in range(128):  # <-- modify the number of generation steps here\n",
        "    # run model with shared cache\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
        "        logits = logits_processor(next_inputs['input_ids'], logits)\n",
        "        new_tokens = torch.multinomial(logits.softmax(dim=-1), 1).flatten(\n",
        "        ) if model.generation_config.do_sample else logits.argmax(-1)\n",
        "\n",
        "    # process generated tokens for printing; handle step change, update next_inputs\n",
        "    assert len(new_tokens) == len(Formatting.workers)\n",
        "    next_input_tokens = new_tokens.unsqueeze(-1).tolist()\n",
        "    for worker_index, (worker_name, worker_tokens, new_token) in enumerate(\n",
        "            zip(Formatting.workers, current_step_tokens_by_worker, new_tokens.tolist())):\n",
        "        worker_tokens.append(new_token)\n",
        "        if Formatting.is_end_of_step(worker_tokens):\n",
        "            # worker just finished their step - add it to common history and start a new step\n",
        "            current_step_index_by_worker[worker_index] += 1\n",
        "            history.extend(worker_tokens)\n",
        "            worker_tokens.clear()\n",
        "            start_msg = Formatting.get_step_prefix(worker_name, current_step_index_by_worker[worker_index])\n",
        "            if tokens_since_last_wait > insert_s1_prompt_every_tokens:\n",
        "                start_msg += Formatting.s1_collab_message   # <-- insert \"Wait, am I doing redundant work?\"\n",
        "                tokens_since_last_wait = 0\n",
        "            worker_tokens.extend(tokenizer.encode(start_msg, add_special_tokens=False))\n",
        "            cache_common.append_from(cm.cache_structure[worker_index][-1])\n",
        "            cm.cache_structure[worker_index][-1].clear()\n",
        "            next_input_tokens[worker_index] = [new_token] + worker_tokens\n",
        "        tokens_since_last_wait += len(next_input_tokens[worker_index])\n",
        "    next_inputs = tokenizer.pad(dict(input_ids=next_input_tokens), padding_side='left', return_tensors='pt').to(device)\n",
        "\n",
        "    if inference_step % print_every_steps == 0:\n",
        "        clear_output(True)  # # display current progress\n",
        "        output_parts = [f\"[**Problem:** {problem}]\\n\\n\"]\n",
        "        output_parts.append(Formatting.history_header + Formatting.SEP + tokenizer.decode(history))\n",
        "        output_parts.append(Formatting.current_step_header)\n",
        "        for worker_index, worker_tokens in enumerate(current_step_tokens_by_worker):\n",
        "            output_parts.append(tokenizer.decode(worker_tokens) + Formatting.pivot_message + Formatting.SEP)\n",
        "        display(Markdown(''.join(output_parts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "uyS3sNHMF_j8",
        "outputId": "b722c848-1f5f-4dac-e178-90b8965b770b"
      },
      "id": "uyS3sNHMF_j8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[**Problem:** Calculate x - x^2 + x * (1 - x) for x = 4,5,6,7.]\n\n### Past steps\n\n**Bob [1]:** Hi, I'm Bob. Let me suggest that we start by calculating the expression for each value of x from 4 to 7.\n\n**Bob [2]:**  Hi, I'm Bob. I'll compute the expression x * (1 - x) for each x from 4 to 7.\n\n**Alice [1]:** Hi, I'm Alice. Here's how we should collaborate: We can split the problem into subtasks. I'll calculate the expression x - x^2 for each x from 4 to 7, and you can calculate x * (1 - x) for each x from 4 to 7. Then we can add those results together.\n\n**Alice [2]:**  Let me compute x - x^2 for x = 4: 4 - 4^2 = 4 - 16 = -12.\n\n**Alice [3]:**  Now that Bob has started, I'll calculate x - x^2 for x = 5: 5 - 5^2 = 5 - 25 = -20.\n\n\n\n### Work in progress (others)\n\n**Alice [4]:**  For<...>\n\n**Bob [3]:**  Hi, I'm Bob. Let me calculate x * (1 - x) for each x from 4 to 7: For x=4, it's 4 * (1-4) = -12. For x=5, it's 5 * (1-5) = -20. For x=6, it's 6 * (1-<...>\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer: small models are poor collaborators** and may be incapable of more complex interactions required for LIMO - and sometimes fail to keep their own promises, doing redundant work despite agreeing not to. We recommend using larger models such as [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) when possible - they work significantly better together."
      ],
      "metadata": {
        "id": "OtFO0CaID5DI"
      },
      "id": "OtFO0CaID5DI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
