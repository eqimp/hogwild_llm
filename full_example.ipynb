{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Parallelism: example with interleaved cache and full prompt\n",
    "\n",
    "This is a more advanced version of `basic_example.ipynb` that features a combined layout: interleaved steps with instant (token-level) synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadbc3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dac5d2c6cf40f786cac2d6a2cfed6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from utils import get_math_input_prompts, get_logits_processor\n",
    "from IPython.display import clear_output, display, Markdown\n",
    "\n",
    "MODEL_NAME = \"Qwen/QwQ-32B\"  # for 48gb gpu, use \"Qwen/QwQ-32B-AWQ\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f9684c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problem = \"\"\"\n",
    "Three vertices of a cube are $P=(7,12,10)$ , $Q=(8,8,1)$ , and $R=(11,3,9)$ . What is the surface area of the cube?\n",
    "\"\"\".strip()\n",
    "\n",
    "print_every_steps = 3\n",
    "insert_s1_prompt_every_tokens = 256\n",
    "tokens_since_last_wait = 0\n",
    "\n",
    "workers = [\"Alice\", \"Bob\"]\n",
    "Formatting = get_math_input_prompts(tokenizer, workers)  # <-- prompts are defined here\n",
    "worker_prompts = [\n",
    "    f\"\"\"{Formatting.get_step_prefix(workers[0], 1)}Hi, I'm {workers[0]}. Here's how we should do this:\"\"\",\n",
    "    f\"\"\"{Formatting.get_step_prefix(workers[1], 1)}Hi, I'm {workers[1]}.\"\"\"\n",
    "]\n",
    "\n",
    "# define cache structure for the combined layout\n",
    "cache_common, cache_current_step_header, cache_separator, cache_w1, cache_w2 = (\n",
    "    shared_cache.CacheBlock(config=model.config) for _ in range(5))\n",
    "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
    "    [cache_common, cache_current_step_header, cache_w2, cache_separator, cache_w1],\n",
    "    [cache_common, cache_current_step_header, cache_w1, cache_separator, cache_w2],\n",
    "])\n",
    "\n",
    "logits_processor = get_logits_processor(model, Formatting.forbidden_token_ix)\n",
    "tokenizer_kwargs = dict(return_tensors='pt', padding=True, padding_side='left', add_special_tokens=False)\n",
    "\n",
    "# initialize generation state for printing\n",
    "history = []\n",
    "current_step_index_by_worker = [1, 1]\n",
    "current_step_tokens_by_worker = [tokenizer.encode(p, add_special_tokens=False) for p in worker_prompts]\n",
    "\n",
    "# pre-fill common parts\n",
    "with torch.inference_mode():\n",
    "    model(**tokenizer([Formatting.get_full_prompt(problem)], **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_common);  # <-- write to common prompt\n",
    "    model(**tokenizer(Formatting.current_step_header, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_current_step_header);   # <-- write to separator\n",
    "    model(**tokenizer(Formatting.current_worker_header, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_separator);   # <-- write to separator between incomplete steps\n",
    "    \n",
    "next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aeff2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[**Problem:** Three vertices of a cube are $P=(7,12,10)$ , $Q=(8,8,1)$ , and $R=(11,3,9)$ . What is the surface area of the cube?]\n",
       "\n",
       "### Past steps\n",
       "\n",
       "**Bob [1]:** Hi, I'm Bob. Let me suggest that we first find the vectors between the points and check if they are edges or face diagonals of the cube. Since all edges are equal and angles between edges are 90 degrees, we can use vector dot products.\n",
       "\n",
       "**Alice [1]:** Hi, I'm Alice. Here's how we should do this: Let's compute the distances between the three points to see if they can be edges, face diagonals, or space diagonals. Since in a cube, edges are equal, face diagonals are edge * sqrt(2), and space diagonals are edge * sqrt(3). So first, let me compute PQ, QR, and RP.\n",
       "\n",
       "\n",
       "\n",
       "### Work in progress (others)\n",
       "\n",
       "**Alice [2]:**  Wait, Bob is already calculating PQ. Maybe I'll compute QR instead. QR is between Q(8,8,1) and R(11,3,9). The differences are (3, -5, 8). Squared distance is 3Â² + (-5<...>\n",
       "\n",
       "**Bob [2]:**  Let me start by calculating the distance between P and Q. P is (7,12,10), Q is (8,8,1). The differences in coordinates are (1, -4, -9). So the squared distance is (1)^2 + (-4)^2 + (-9)^2 = 1 + 16 + 81 = 9<...>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inference_step in range(1024):  # <-- modify the number of generation steps here\n",
    "    # run model with shared cache\n",
    "    with torch.inference_mode():\n",
    "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        logits = logits_processor(next_inputs['input_ids'], logits)\n",
    "        new_tokens = torch.multinomial(logits.softmax(dim=-1), 1).flatten(\n",
    "        ) if model.generation_config.do_sample else logits.argmax(-1)\n",
    "    \n",
    "    # process generated tokens for printing; handle step change, update next_inputs\n",
    "    assert len(new_tokens) == len(Formatting.workers)\n",
    "    next_input_tokens = new_tokens.unsqueeze(-1).tolist()    \n",
    "    for worker_index, (worker_name, worker_tokens, new_token) in enumerate(\n",
    "            zip(Formatting.workers, current_step_tokens_by_worker, new_tokens.tolist())):\n",
    "        worker_tokens.append(new_token)\n",
    "        if Formatting.is_end_of_step(worker_tokens):\n",
    "            # worker just finished their step - add it to common history and start a new step\n",
    "            current_step_index_by_worker[worker_index] += 1\n",
    "            history.extend(worker_tokens)\n",
    "            worker_tokens.clear()\n",
    "            start_msg = Formatting.get_step_prefix(worker_name, current_step_index_by_worker[worker_index])\n",
    "            if tokens_since_last_wait > insert_s1_prompt_every_tokens:\n",
    "                start_msg += Formatting.s1_collab_message   # <-- insert \"Wait, am I doing redundant work?\"ii\n",
    "                tokens_since_last_wait = 0\n",
    "            worker_tokens.extend(tokenizer.encode(start_msg, add_special_tokens=False))\n",
    "            cache_common.append_from(cm.cache_structure[worker_index][-1])\n",
    "            cm.cache_structure[worker_index][-1].clear()\n",
    "            next_input_tokens[worker_index] = [new_token] + worker_tokens\n",
    "        tokens_since_last_wait += len(next_input_tokens[worker_index])\n",
    "    next_inputs = tokenizer.pad(dict(input_ids=next_input_tokens), padding_side='left', return_tensors='pt').to(device)\n",
    "\n",
    "    if inference_step % print_every_steps == 0:\n",
    "        clear_output(True)  # # display current progress\n",
    "        output_parts = [f\"[**Problem:** {problem}]\\n\\n\"]\n",
    "        output_parts.append(Formatting.history_header + Formatting.SEP + tokenizer.decode(history))\n",
    "        output_parts.append(Formatting.current_step_header)\n",
    "        for worker_index, worker_tokens in enumerate(current_step_tokens_by_worker):\n",
    "            output_parts.append(tokenizer.decode(worker_tokens) + Formatting.pivot_message + Formatting.SEP)\n",
    "        display(Markdown(''.join(output_parts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
