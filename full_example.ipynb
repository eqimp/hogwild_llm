{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2e251b",
   "metadata": {},
   "source": [
    "# Hogwild! Parallelism: example with interleaved cache and full prompt\n",
    "\n",
    "This is a more advanced version of `basic_example.ipynb` that features a combined layout: interleaved steps with instant (token-level) synchronization. You can find a more script-friendly version of this code in [__`./generation.py`__](./generation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadbc3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1a126e91194f1998408e357d5f9134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import shared_cache\n",
    "from generation import MathFormatting, get_logits_processor\n",
    "from IPython.display import clear_output, display, Markdown\n",
    "\n",
    "MODEL_NAME = \"Qwen/QwQ-32B\"  # for 48gb gpu, use \"Qwen/QwQ-32B-AWQ\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype='auto', low_cpu_mem_usage=True, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f9684c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "problem = \"\"\"\n",
    "Three vertices of a cube are $P=(7,12,10)$ , $Q=(8,8,1)$ , and $R=(11,3,9)$ . What is the surface area of the cube?\n",
    "\"\"\".strip()\n",
    "\n",
    "print_every_steps = 3\n",
    "insert_s1_prompt_every_tokens = 256\n",
    "tokens_since_last_wait = 0\n",
    "\n",
    "workers = [\"Alice\", \"Bob\"]\n",
    "fmt = MathFormatting(\n",
    "    tokenizer, workers, prompt_style=\"second_person\", add_suggestions_on_collaborating=True, add_examples=True,\n",
    ")  # ^-- prompts and optional few-shot examples; has options for different model types - see formatting.py\n",
    "worker_prompts = [\n",
    "    f\"\"\"{fmt.get_step_prefix(workers[0], 1)}Hi, I'm {workers[0]}. Here's how we can collaborate\"\"\",\n",
    "    f\"\"\"{fmt.get_step_prefix(workers[1], 1)}Hi, I'm {workers[1]}.\"\"\"\n",
    "]\n",
    "\n",
    "# define cache structure for the combined layout\n",
    "cache_common, cache_current_step_header, cache_separator, cache_w1, cache_w2 = (\n",
    "    shared_cache.CacheBlock(config=model.config) for _ in range(5))\n",
    "cm = shared_cache.SharedCacheManager(cache_structure=[\n",
    "    [cache_common, cache_current_step_header, cache_w2, cache_separator, cache_w1],\n",
    "    [cache_common, cache_current_step_header, cache_w1, cache_separator, cache_w2],\n",
    "])\n",
    "\n",
    "logits_processor = get_logits_processor(model, fmt.forbidden_token_ix)\n",
    "tokenizer_kwargs = dict(return_tensors='pt', padding=True, padding_side='left', add_special_tokens=False)\n",
    "\n",
    "# initialize generation state for printing\n",
    "history = []\n",
    "current_step_index_by_worker = [1, 1]\n",
    "current_step_tokens_by_worker = [tokenizer.encode(p, add_special_tokens=False) for p in worker_prompts]\n",
    "\n",
    "# pre-fill common parts\n",
    "with torch.inference_mode():\n",
    "    model(**tokenizer([fmt.get_full_prompt(problem)], **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_common);  # <-- write to common prompt\n",
    "    model(**tokenizer(fmt.current_step_header, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_current_step_header);   # <-- write to separator\n",
    "    model(**tokenizer(fmt.current_worker_header, **tokenizer_kwargs).to(device),\n",
    "          use_cache=True, past_key_values=cache_separator);   # <-- write to separator between incomplete steps\n",
    "    \n",
    "next_inputs = tokenizer(worker_prompts, **tokenizer_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aeff2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[**Problem:** Three vertices of a cube are $P=(7,12,10)$ , $Q=(8,8,1)$ , and $R=(11,3,9)$ . What is the surface area of the cube?]\n",
       "\n",
       "### Past steps\n",
       "\n",
       "**Bob [1]:** Hi, I'm Bob. Let's first compute the distances between the points to determine which edges are edges, face diagonals, or space diagonals of the cube.\n",
       "\n",
       "**Bob [2]:**  Okay, Alice, let's split the work. I'll calculate the distance between P and Q, you do Q and R, and then we'll see who does the remaining. \n",
       "\n",
       "**Alice [1]:** Hi, I'm Alice. Here's how we can collaborate: Let's start by calculating the distances between each pair of points to see which are edges, face diagonals, or space diagonals. Since it's a cube, edges are all equal, face diagonals are edge*sqrt(2), and space diagonals are edge*sqrt(3). Let me start with distance PQ.\n",
       "\n",
       "**Alice [2]:**  Calculating PQ: P(7,12,10) to Q(8,8,1). The differences in coordinates are Δx=1, Δy=-4, Δz=-9. Squaring and adding: 1² + (-4)^2 + (-9)^2 =1 +16+81=98. So PQ distance is sqrt(98)=7√2≈13.28.\n",
       "\n",
       "**Bob [3]:**  I'll compute the distance between P=(7,12,10) and Q=(8,8,1). The differences in coordinates are Δx = 8-7=1, Δy=8-12=-4, Δz=1-10=-9. Squaring each: 1, 16, 81. Sum is 1+16=17, 17+81=98. So PQ distance is sqrt(98)=7√2. \n",
       "\n",
       "**Bob [4]:**  So PQ distance squared is 98. Now moving on, perhaps compute QR next?\n",
       "\n",
       "\n",
       "\n",
       "### Work in progress (others)\n",
       "\n",
       "**Alice [3]:** Quick check: am I doing redundant work? (yes/no):  Wait, Bob is already calculating PQ, so I'll switch to QR. Let me compute QR: Q=(8,8,1) to R=(11,3,9). Differences: Δx=3, Δy=-5, Δz=8. Squared: 9 +25 +64=98 again? Wait, 3²=9, (-5)^2=25, <...>\n",
       "\n",
       "**Bob [5]:**  Alice, I see you switched to QR. Good. I'll compute RP now. RP is from R=(11,3,9) to P=(7,12,10). Differences: Δx= -4, Δy=9, Δ<...>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for inference_step in range(1024):  # <-- modify the number of generation steps here\n",
    "    # run model with shared cache\n",
    "    with torch.inference_mode():\n",
    "        logits = model(**cm.get_input_kwargs(**next_inputs)).logits[..., -1, :]\n",
    "        logits = logits_processor(next_inputs['input_ids'], logits)\n",
    "        new_tokens = torch.multinomial(logits.softmax(dim=-1), 1).flatten(\n",
    "        ) if model.generation_config.do_sample else logits.argmax(-1)\n",
    "    \n",
    "    # process generated tokens for printing; handle step change, update next_inputs\n",
    "    assert len(new_tokens) == len(fmt.workers)\n",
    "    next_input_tokens = new_tokens.unsqueeze(-1).tolist()    \n",
    "    for worker_index, (worker_name, worker_tokens, new_token) in enumerate(\n",
    "            zip(fmt.workers, current_step_tokens_by_worker, new_tokens.tolist())):\n",
    "        worker_tokens.append(new_token)\n",
    "        if fmt.is_end_of_step(worker_tokens):\n",
    "            # worker just finished their step - add it to common history and start a new step\n",
    "            current_step_index_by_worker[worker_index] += 1\n",
    "            history.extend(worker_tokens)\n",
    "            worker_tokens.clear()\n",
    "            start_msg = fmt.get_step_prefix(worker_name, current_step_index_by_worker[worker_index])\n",
    "            if tokens_since_last_wait > insert_s1_prompt_every_tokens:\n",
    "                start_msg += fmt.s1_collab_message   # <-- insert \"Wait, am I doing redundant work?\"\n",
    "                tokens_since_last_wait = 0\n",
    "            worker_tokens.extend(tokenizer.encode(start_msg, add_special_tokens=False))\n",
    "            cache_common.append_from(cm.cache_structure[worker_index][-1])\n",
    "            cm.cache_structure[worker_index][-1].clear()\n",
    "            next_input_tokens[worker_index] = [new_token] + worker_tokens\n",
    "        tokens_since_last_wait += len(next_input_tokens[worker_index])\n",
    "    next_inputs = tokenizer.pad(dict(input_ids=next_input_tokens), padding_side='left', return_tensors='pt').to(device)\n",
    "\n",
    "    if inference_step % print_every_steps == 0:\n",
    "        clear_output(True)  # display current progress\n",
    "        output_parts = [f\"[**Problem:** {problem}]\\n\\n\"]\n",
    "        output_parts.append(fmt.history_header + fmt.sep + tokenizer.decode(history))\n",
    "        output_parts.append(fmt.current_step_header)\n",
    "        for worker_index, worker_tokens in enumerate(current_step_tokens_by_worker):\n",
    "            output_parts.append(tokenizer.decode(worker_tokens) + fmt.pivot_message + fmt.sep)\n",
    "        display(Markdown(''.join(output_parts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
